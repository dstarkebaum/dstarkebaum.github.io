import json
import os
import time
import sys
import csv
import argparse
from contextlib import ExitStack

# global variables are frowned upon... probably there is a better way for this...
# delimiter for CSV files
d = '|'
tables = ['papers', 'is_cited_by', 'cites', 'authors', 'has_author', 'is_author_of']

# eliminate newlines, quotations, delimiters, and extra white space
def clean(text):
    return text.replace(d,'').replace('\n','').replace('"','').replace("'",'').strip()
# turn a list into a CSV row
def format(list):
    return d.join(list)+'\n'
# generate a path string from a single file name

def to_secs(time):
    return "{:0.4f}".format(time)

def parse_args(args):
    parser = argparse.ArgumentParser()
    parser.add_argument('json_src',
            help='Filename (including path) of the json file to parse')
    parser.add_argument('-i,','--int',help='convert id from hex to int',
            action='store_true')
    parser.add_argument('--unique',help='make each csv file unique by including the json_src name',
            action='store_true')
    parser.add_argument('-p','--path',type=str,default='data/csv',
            help='relative path to store the output csv files')
    return parser.parse_args()

def main(sys_args):
    args = parse_args(sys_args)

    corpus_path = os.getcwd()+'/'+args.json_src
    # Input File (1.5GB JSON)

    src_file = os.path.basename(corpus_path)

    make_int=False
    if args.int:
        print('Storing ids as big integers ~ Order(10^49)')
        make_int = true

    # make your own damn directories!
    #dirs = args.path.split('/')
    #for i in range(len(dirs)):
    #    if not os.path.exists(dirs[i]):
    #        os.makedirs(dir)

    output_dir = args.path

    if args.unique:
        print('creating unique filename like: json_src-table_name.csv')

    print('Exporting to ' + output_dir)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    parse_json(corpus_path, output_dir, make_int, args.unique)

'''
Combine output_dir, table_name, and src_file name into a complete (absolute) path string
of the form: /absolute/path/to/output_dir/table_name.csv
if unique=True is passed, the name of the json src file is also included:
    /absolute/path/to/output_dir/src_file-table_name.csv
'''
def path(table_name, output_dir, src_file='', unique=False):
    filename = ''

    if unique:
        filename = src_file+"-"+table_name+'.csv'
    else:
        filename = table_name+'.csv'
    return(os.path.join(os.getcwd(),output_dir,filename))

def parse_json(corpus_path, output_dir, src_file, make_int=False,unique=False):
    src_file = os.path.basename(corpus_path)
    # Print status to STDOUT
    start_time = time.perf_counter()
    print("Parsing: "+corpus_path)
    print("Start time: "+time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))

    output_files = {t:path(t,output_dir,src_file) for t in tables}

    # keep a count of the number of records parsed
    count = 0

    with open(corpus_path, 'r') as json_in:
        with ExitStack() as stack:
            files = { t:stack.enter_context(open(output_files[t],'w')) for t in tables}

            # Parse each line of JSON, and write the appropriate fields
            # to each csv table
            for line in json_in:

                #if 10000 == count:
                #    break

                # test with 100 lines to start
                if count in [100, 1000, 10000, 100000, 500000]:
                    print(str(count)+" records parsed after "+to_secs(time.perf_counter() - start_time)+" seconds")
                count = count + 1

                js_line = json.loads(line)

                id = clean(js_line['id'])
                if make_int:
                    id = str(int(id,16))
                title = clean(js_line['title'])
                doi = clean(js_line['doi'])

                #abstract = clean(js_line['paperAbstract'])

                # Note: js_line['s2Url'] is redundant
                # It can be generated by: https://semanticscholar.org/paper/'id'

                # some entries seem to be missing a year, so just use an empty string
                try:
                    year = clean(str(js_line['year']))
                except KeyError:
                    year = ''

                paper_record = [id,title,year,doi]#, abstract]

                files['papers'].write(format(paper_record))

                # each JSON row conains a list of citations and authors
                is_cited_by_list = js_line['inCitations']
                cites_list = js_line['outCitations']
                authors_list = js_line['authors']


                # inCitations and outCitations need to go to their own tables
                for cit in is_cited_by_list:
                    cit=clean(cit)
                    if make_int:
                        cit = str(int(cit,16))
                    files['is_cited_by'].write(format([id,cit]))
                for cit in cites_list:
                    if make_int:
                        cit = str(int(cit,16))
                    cit=clean(cit)
                    files['cites'].write(format([id,cit]))

                author_set = set()
                # js_line['authors'] has a format like:
                # [{"name":"Huseyin Demirbilek","ids":["4800055"]},{"name":"Serhan KÃ¼peli","ids":["5942490"]}]
                # So apparently each name can be associated with multiple ids
                # We are only insterested in the first one, which should be unique
                for author in authors_list:
                    # Some papers have no authors.
                    # If there are no authors for a paper, skip to the next one
                    if 0 == len(author['ids']):
                        continue
                    #print(author['ids'][0])
                    #author_id_int = int(author['ids'][0])

                    author_id = clean(author['ids'][0])
                    author_name = clean(author['name'])

                    if author_id not in author_set:
                        author_set.add(author_id)
                        files['authors'].write(format([author_id,author_name]))
                        files['has_author'].write(format([id,author_id]))
                        files['is_author_of'].write(format([author_id,id]))

    print(str(count)+" records written to csv after "+to_secs(time.perf_counter() - start_time)+" seconds")

if __name__=="__main__":
    main(sys.argv[1:])
